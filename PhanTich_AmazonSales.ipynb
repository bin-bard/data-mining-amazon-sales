{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0fdc8a",
   "metadata": {},
   "source": [
    "## M·ª•c ƒê√≠ch Ph√¢n C·ª•m v√† C√¢u H·ªèi Nghi√™n C·ª©u\n",
    "\n",
    "**M·ª•c ti√™u ch√≠nh:** X√°c ƒë·ªãnh v√† ph√¢n t√≠ch c√°c ph√¢n kh√∫c s·∫£n ph·∫©m tr√™n Amazon d·ª±a tr√™n m·ªëi quan h·ªá gi·ªØa c√°c y·∫øu t·ªë: ƒë·∫∑c t√≠nh/t√≠nh nƒÉng m√¥ t·∫£ c·ªßa s·∫£n ph·∫©m, gi√° b√°n v√† ƒë√°nh gi√° t·ª´ ng∆∞·ªùi d√πng. Qua ƒë√≥, t√¨m hi·ªÉu xem c√°c s·∫£n ph·∫©m ƒëang ƒë∆∞·ª£c ƒë·ªãnh v·ªã nh∆∞ th·∫ø n√†o tr√™n th·ªã tr∆∞·ªùng v·ªÅ m·∫∑t \"gi√° tr·ªã\" v√† \"ch·∫•t l∆∞·ª£ng\".\n",
    "\n",
    "**T·∫°i sao ch·ªçn c√°c thu·ªôc t√≠nh n√†y?**\n",
    "\n",
    "* **`about_product` (M√¥ t·∫£ s·∫£n ph·∫©m - D·ªØ li·ªáu VƒÉn b·∫£n):**\n",
    "    * **ƒê√≥ng g√≥p:** Cung c·∫•p th√¥ng tin chi ti·∫øt v·ªÅ c√°c t√≠nh nƒÉng, l·ª£i √≠ch, c√¥ng d·ª•ng, v√† ƒë·∫∑c ƒëi·ªÉm k·ªπ thu·∫≠t c·ªßa s·∫£n ph·∫©m. ƒê√¢y l√† y·∫øu t·ªë then ch·ªët ƒë·ªÉ hi·ªÉu ƒë∆∞·ª£c \"gi√° tr·ªã n·ªôi t·∫°i\" ho·∫∑c \"ch·∫•t l∆∞·ª£ng ƒë∆∞·ª£c qu·∫£ng b√°\" c·ªßa s·∫£n ph·∫©m.\n",
    "    * **Vai tr√≤ trong ph√¢n c·ª•m:** Gi√∫p nh√≥m c√°c s·∫£n ph·∫©m c√≥ b·ªô t√≠nh nƒÉng t∆∞∆°ng t·ª± ho·∫∑c h∆∞·ªõng ƒë·∫øn c√πng m·ªôt nhu c·∫ßu s·ª≠ d·ª•ng.\n",
    "\n",
    "* **`discounted_price` / `actual_price` (Gi√° s·∫£n ph·∫©m - D·ªØ li·ªáu S·ªë):**\n",
    "    * **ƒê√≥ng g√≥p:** Ph·∫£n √°nh chi ph√≠ m√† ng∆∞·ªùi ti√™u d√πng ph·∫£i b·ªè ra ƒë·ªÉ s·ªü h·ªØu s·∫£n ph·∫©m.\n",
    "    * **Vai tr√≤ trong ph√¢n c·ª•m:** L√† m·ªôt tr·ª•c quan tr·ªçng ƒë·ªÉ so s√°nh \"gi√° tr·ªã nh·∫≠n ƒë∆∞·ª£c\" so v·ªõi \"chi ph√≠ b·ªè ra\".\n",
    "\n",
    "* **`rating` (ƒê√°nh gi√° trung b√¨nh - D·ªØ li·ªáu S·ªë):**\n",
    "    * **ƒê√≥ng g√≥p:** Th·ªÉ hi·ªán m·ª©c ƒë·ªô h√†i l√≤ng c·ªßa kh√°ch h√†ng ƒë√£ s·ª≠ d·ª•ng s·∫£n ph·∫©m, m·ªôt th∆∞·ªõc ƒëo quan tr·ªçng v·ªÅ \"ch·∫•t l∆∞·ª£ng c·∫£m nh·∫≠n\" ho·∫∑c \"ch·∫•t l∆∞·ª£ng th·ª±c t·∫ø\" sau khi tr·∫£i nghi·ªám.\n",
    "    * **Vai tr√≤ trong ph√¢n c·ª•m:** Gi√∫p x√°c ƒë·ªãnh m·ª©c ƒë·ªô ch·∫•p nh·∫≠n v√† y√™u th√≠ch c·ªßa th·ªã tr∆∞·ªùng ƒë·ªëi v·ªõi s·∫£n ph·∫©m.\n",
    "\n",
    "**C√°c c√¢u h·ªèi m√† vi·ªác ph√¢n c·ª•m v·ªõi c√°c thu·ªôc t√≠nh n√†y nh·∫±m tr·∫£ l·ªùi:**\n",
    "\n",
    "1.  **Ph√¢n kh√∫c s·∫£n ph·∫©m theo gi√° tr·ªã v√† ch·∫•t l∆∞·ª£ng:**\n",
    "    * C√≥ th·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c nh√≥m s·∫£n ph·∫©m n√†o mang l·∫°i **\"gi√° tr·ªã t·ªët nh·∫•t\"** (v√≠ d·ª•: nhi·ªÅu t√≠nh nƒÉng ƒë∆∞·ª£c m√¥ t·∫£, rating cao nh∆∞ng gi√° c·∫£ ph·∫£i chƒÉng) kh√¥ng?\n",
    "    * C√≥ th·ªÉ ph√¢n bi·ªát ƒë∆∞·ª£c nh√≥m s·∫£n ph·∫©m **\"cao c·∫•p\"** (v√≠ d·ª•: m√¥ t·∫£ nhi·ªÅu t√≠nh nƒÉng v∆∞·ª£t tr·ªôi, rating r·∫•t cao, v√† gi√° cao t∆∞∆°ng x·ª©ng) v·ªõi nh√≥m s·∫£n ph·∫©m **\"c∆° b·∫£n/gi√° r·∫ª\"** (v√≠ d·ª•: m√¥ t·∫£ √≠t t√≠nh nƒÉng, rating ·ªü m·ª©c ch·∫•p nh·∫≠n ƒë∆∞·ª£c, gi√° th·∫•p) kh√¥ng?\n",
    "\n",
    "2.  **Ph√°t hi·ªán c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát v·ªÅ ƒë·ªãnh gi√° v√† c·∫£m nh·∫≠n ch·∫•t l∆∞·ª£ng:**\n",
    "    * C√≥ nh√≥m s·∫£n ph·∫©m n√†o d∆∞·ªùng nh∆∞ ƒë∆∞·ª£c **\"ƒë·ªãnh gi√° qu√° cao\"** (v√≠ d·ª•: m√¥ t·∫£ nhi·ªÅu t√≠nh nƒÉng nh∆∞ng rating th·ª±c t·∫ø th·∫•p so v·ªõi m·ª©c gi√°) kh√¥ng?\n",
    "    * C√≥ th·ªÉ t√¨m ra nh·ªØng **\"m√≥n h·ªùi ti·ªÅm ·∫©n\"** (v√≠ d·ª•: rating t·ªët, gi√° th·∫•p, nh∆∞ng c√≥ th·ªÉ ph·∫ßn m√¥ t·∫£ ch∆∞a l√†m n·ªïi b·∫≠t h·∫øt c√°c t√≠nh nƒÉng gi√° tr·ªã) kh√¥ng?\n",
    "\n",
    "3.  **Hi·ªÉu c·∫•u tr√∫c th·ªã tr∆∞·ªùng:**\n",
    "    * C√°c c·ª•m s·∫£n ph·∫©m h√¨nh th√†nh c√≥ t∆∞∆°ng quan nh∆∞ th·∫ø n√†o gi·ªØa ba y·∫øu t·ªë: t√≠nh nƒÉng m√¥ t·∫£, gi√° c·∫£, v√† ƒë√°nh gi√° c·ªßa ng∆∞·ªùi d√πng? (V√≠ d·ª•: M·ªôt c·ª•m c√≥ th·ªÉ l√† \"c√°c s·∫£n ph·∫©m ƒëi·ªán t·ª≠ v·ªõi m√¥ t·∫£ nhi·ªÅu t√≠nh nƒÉng, gi√° cao v√† rating r·∫•t t·ªët\", trong khi m·ªôt c·ª•m kh√°c l√† \"ph·ª• ki·ªán gi√° r·∫ª, m√¥ t·∫£ ƒë∆°n gi·∫£n, rating ·ªü m·ª©c kh√°\").\n",
    "\n",
    "B·∫±ng c√°ch ph√¢n c·ª•m d·ª±a tr√™n s·ª± k·∫øt h·ª£p c·ªßa ba thu·ªôc t√≠nh n√†y, ch√∫ng ta k·ª≥ v·ªçng s·∫Ω kh√°m ph√° ra nh·ªØng ph√¢n kh√∫c s·∫£n ph·∫©m c√≥ √Ω nghƒ©a, t·ª´ ƒë√≥ cung c·∫•p c√°i nh√¨n s√¢u s·∫Øc h∆°n v·ªÅ chi·∫øn l∆∞·ª£c s·∫£n ph·∫©m v√† ƒë·ªãnh v·ªã gi√° tr√™n th·ªã tr∆∞·ªùng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# --- M·ªöI TH√äM V√ÄO: Th∆∞ vi·ªán cho m√¥ h√¨nh h·ªìi quy v√† ph√¢n lo·∫°i ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n", # Th√™m tensorflow
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# C·∫•u h√¨nh hi·ªÉn th·ªã\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# C·∫•u h√¨nh c·∫£nh b√°o\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "4b9705de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"data/amazon.csv\")\n",
    "df = df_raw.copy() # T·∫°o b·∫£n sao ƒë·ªÉ l√†m vi·ªác, gi·ªØ df_raw nguy√™n v·∫πn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf5209",
   "metadata": {},
   "source": [
    "# 1. M√î T·∫¢ T·∫¨P D·ªÆ LI·ªÜU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1619301",
   "metadata": {},
   "source": [
    "### üì¶ Th√¥ng tin v·ªÅ t·∫≠p Amazon Sales Dataset\n",
    "#### üõçÔ∏è Th√¥ng tin v·ªÅ s·∫£n ph·∫©m\n",
    "\n",
    "| Tr∆∞·ªùng              | M√¥ t·∫£                                                                 |\n",
    "|---------------------|----------------------------------------------------------------------|\n",
    "| `product_id`        | M√£ ƒë·ªãnh danh duy nh·∫•t c·ªßa s·∫£n ph·∫©m.                                  |\n",
    "| `product_name`      | T√™n s·∫£n ph·∫©m.                                                        |\n",
    "| `category`          | Danh m·ª•c s·∫£n ph·∫©m.                                                   |\n",
    "| `discounted_price`  | Gi√° sau khi gi·∫£m (hi·ªÉn th·ªã v·ªõi kh√°ch h√†ng).                          |\n",
    "| `actual_price`      | Gi√° g·ªëc c·ªßa s·∫£n ph·∫©m tr∆∞·ªõc khi gi·∫£m gi√°.                             |\n",
    "| `discount_percentage` | Ph·∫ßn trƒÉm gi·∫£m gi√° ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n `actual_price`.              |\n",
    "| `rating`            | ƒêi·ªÉm ƒë√°nh gi√° trung b√¨nh t·ª´ ng∆∞·ªùi d√πng (thang ƒëi·ªÉm 5).               |\n",
    "| `rating_count`      | S·ªë l∆∞·ª£ng ng∆∞·ªùi ƒë√£ ƒë√°nh gi√° s·∫£n ph·∫©m.                                 |\n",
    "| `about_product`     | M√¥ t·∫£ ng·∫Øn ho·∫∑c th√¥ng tin k·ªπ thu·∫≠t v·ªÅ s·∫£n ph·∫©m.                      |\n",
    "| `img_link`          | Link h√¨nh ·∫£nh ƒë·∫°i di·ªán c·ªßa s·∫£n ph·∫©m.                                 |\n",
    "| `product_link`      | Link t·ªõi trang ch√≠nh th·ª©c c·ªßa s·∫£n ph·∫©m tr√™n website (Amazon ho·∫∑c kh√°c). |\n",
    "\n",
    "#### üßë Th√¥ng tin ƒë√°nh gi√° t·ª´ ng∆∞·ªùi d√πng\n",
    "\n",
    "| Tr∆∞·ªùng           | M√¥ t·∫£                                                                 |\n",
    "|------------------|----------------------------------------------------------------------|\n",
    "| `user_id`        | M√£ ƒë·ªãnh danh c·ªßa ng∆∞·ªùi d√πng ƒë√£ ƒë√°nh gi√° s·∫£n ph·∫©m.                    |\n",
    "| `user_name`      | T√™n ng∆∞·ªùi d√πng ƒë√£ vi·∫øt ƒë√°nh gi√°. C√≥ th·ªÉ l√† nickname ho·∫∑c t√™n th·∫≠t.   |\n",
    "| `review_id`      | M√£ ƒë·ªãnh danh c·ªßa ƒë√°nh gi√°, d√πng ƒë·ªÉ ph√¢n bi·ªát t·ª´ng b√†i review.        |\n",
    "| `review_title`   | Ti√™u ƒë·ªÅ ng·∫Øn g·ªçn c·ªßa ƒë√°nh gi√°, th·ªÉ hi·ªán √Ω ch√≠nh.                     |\n",
    "| `review_content` | N·ªôi dung chi ti·∫øt c·ªßa b√†i ƒë√°nh gi√° t·ª´ ng∆∞·ªùi d√πng.                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b1880",
   "metadata": {},
   "source": [
    "# 2. ƒê√ÅNH GI√Å T·∫¨P D·ªÆ LI·ªÜU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "16302653",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "print(\"NƒÉm d√≤ng ƒë·∫ßu ti√™n c·ªßa DataFrame th√¥:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "fefb9710",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "print(f\"\\nK√≠ch th∆∞·ªõc c·ªßa DataFrame th√¥: {df.shape[0]} d√≤ng, {df.shape[1]} c·ªôt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "e2ec0c05",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a21102",
   "metadata": {},
   "source": [
    "Ta th·∫•y d·ªØ li·ªáu g·ªìm c√≥ 1465 d√≤ng v·ªõi 16 c·ªôt d·ªØ li·ªáu d∆∞·ªõi d·∫°ng object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49edd481",
   "metadata": {},
   "source": [
    "C√°c c·ªôt discounted_price, actual_price, discount_percentage, rating, rating_count c·∫ßn chuy·ªÉn v·ªÅ  d·∫°ng s·ªë:\n",
    "| **C·ªôt**              | **Ki·ªÉu m·ªõi** | **Ghi ch√∫**                                                            |\n",
    "|----------------------|--------------|------------------------------------------------------------------------|\n",
    "| `discounted_price`   | `float`      | C·∫ßn lo·∫°i b·ªè k√Ω t·ª± `‚Çπ`, d·∫•u ph·∫©y n·∫øu c√≥ tr∆∞·ªõc khi chuy·ªÉn ƒë·ªïi ki·ªÉu      |\n",
    "| `actual_price`       | `float`      | C·∫ßn lo·∫°i b·ªè k√Ω t·ª± `‚Çπ`, d·∫•u ph·∫©y n·∫øu c√≥ tr∆∞·ªõc khi chuy·ªÉn ƒë·ªïi ki·ªÉu      |\n",
    "| `rating`             | `float`      | ƒê√£ ·ªü d·∫°ng s·ªë th·∫≠p ph√¢n, ch·ªâ c·∫ßn √©p ki·ªÉu n·∫øu c·∫ßn                        |\n",
    "| `discount_percentage`| `float`        | C·∫ßn lo·∫°i b·ªè d·∫•u `%` tr∆∞·ªõc khi √©p ki·ªÉu                                 |\n",
    "| `rating_count`       | `int`        | Lo·∫°i b·ªè d·∫•u ph·∫©y h√†ng ngh√¨n (`,`) tr∆∞·ªõc khi √©p ki·ªÉu                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0cff2",
   "metadata": {},
   "source": [
    "C√°c c·ªôt product_name, category, about_product, review_title, review_content c·∫ßn x·ª≠ l√Ω d·∫°ng vƒÉn b·∫£n, r√∫t g·ªçn n·ªôi dung, t√™n s·∫£n ph·∫©m/ danh m·ª•c: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e206a52b",
   "metadata": {},
   "source": [
    "| **C·ªôt**             | **C√°ch x·ª≠ l√Ω**                                                                                                                                                                  |\n",
    "|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `product_name`      | R√∫t g·ªçn t√™n s·∫£n ph·∫©m, lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát                                                                                 |\n",
    "| `category`          | ƒê√¢y l√† chu·ªói ph√¢n c·∫•p danh m·ª•c, v√≠ d·ª•: `Computers&Accessories\\|Accessories&Peripherals\\|...\\|USBCables`. C√≥ th·ªÉ x·ª≠ l√Ω theo c√°c c√°ch:                                          |\n",
    "|                     | - Tr√≠ch xu·∫•t danh m·ª•c **c·ª• th·ªÉ nh·∫•t** (v√≠ d·ª•: `USBCables`)                                                                                                                      |\n",
    "|                     | - K·∫øt h·ª£p danh m·ª•c **ch√≠nh v√† cu·ªëi c√πng** (v√≠ d·ª•: `Computers&Accessories - USBCables`)                                                                                          |\n",
    "|                     | - Ho·∫∑c t√°ch th√†nh **c√°c c·ªôt ri√™ng bi·ªát** theo t·ª´ng c·∫•p ph√¢n lo·∫°i (v√≠ d·ª•: `cat_lvl_1`, `cat_lvl_2`, ...)                                                                        |\n",
    "| `about_product`     | R√∫t g·ªçn n·ªôi dung m√¥ t·∫£ ho·∫∑c √°p d·ª•ng c√°c k·ªπ thu·∫≠t x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP): lo·∫°i b·ªè t·ª´ d·ª´ng, chu·∫©n ho√° vƒÉn b·∫£n, t√≥m t·∫Øt n·ªôi dung.                                          |\n",
    "| `review_title`      | L√†m s·∫°ch d·∫•u c√¢u, lo·∫°i b·ªè tr√πng l·∫∑p, chu·∫©n ho√° ch·ªØ th∆∞·ªùng/hoa.                                  |\n",
    "| `review_content`    | T∆∞∆°ng t·ª± nh∆∞ `review_title`: l√†m s·∫°ch, chu·∫©n ho√°, c√≥ th·ªÉ d√πng `t√≥m t·∫Øt`, `TF-IDF`, `Word2Vec` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "dbbf6b12",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "print(\"\\nGi√° tr·ªã null ·ªü c√°c c·ªôt:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "6cc1d0d0",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# Ph·∫ßn trƒÉm gi√° tr·ªã null ·ªü c√°c c·ªôt\n",
    "print(\"\\nPh·∫ßn trƒÉm gi√° tr·ªã null ·ªü c√°c c·ªôt:\")\n",
    "print(df.isnull().mean().round(4) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a2fd7",
   "metadata": {},
   "source": [
    "C√≥ c·ªôt rating_count c√≥ t·ª∑ l·ªá % Null l√† 0.14%, c√≥ th·ªÉ x√≥a/b·ªè ƒëi ƒë∆∞·ª£c. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23610b3e",
   "metadata": {},
   "source": [
    "# 3. TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d6064",
   "metadata": {},
   "source": [
    "## 3.1. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu s·ªë "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "5cc0e657",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# T·∫°o m·ªôt b·∫£n sao ƒë·ªÉ thao t√°c an to√†n\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ph·∫£i s·ªë kh·ªèi c√°c c·ªôt gi√°\n",
    "price_columns = ['discounted_price', 'actual_price', 'discount_percentage', 'rating_count']\n",
    "for col in price_columns:\n",
    "    # ƒê·∫£m b·∫£o c·ªôt l√† string tr∆∞·ªõc khi d√πng .str\n",
    "    df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed[col] = df_processed[col].str.replace('‚Çπ', '', regex=False)\n",
    "    df_processed[col] = df_processed[col].str.replace(',', '', regex=False)\n",
    "    df_processed[col] = df_processed[col].str.replace('%', '', regex=False)\n",
    "\n",
    "print(\"ƒê√£ lo·∫°i b·ªè k√Ω t·ª± kh·ªèi c√°c c·ªôt gi√°.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "84f6757b",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "c7432b90",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# Chuy·ªÉn ƒë·ªïi c√°c c·ªôt sang d·∫°ng s·ªë\n",
    "numeric_conversion_cols = ['discounted_price', 'actual_price', 'rating', 'discount_percentage', 'rating_count']\n",
    "for col in numeric_conversion_cols:\n",
    "    df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "\n",
    "# X·ª≠ l√Ω ri√™ng discount_percentage v√† rating_count n·∫øu c·∫ßn\n",
    "if 'discount_percentage' in df_processed.columns and df_processed['discount_percentage'].notnull().any():\n",
    "    df_processed['discount_percentage'] = df_processed['discount_percentage']/100\n",
    "if 'rating_count' in df_processed.columns and df_processed['rating_count'].notnull().any():\n",
    "    # Chuy·ªÉn sang Int64 ƒë·ªÉ cho ph√©p gi√° tr·ªã NA (n·∫øu c√≥ sau n√†y) m√† kh√¥ng b·ªã l·ªói √©p ki·ªÉu int th√¥ng th∆∞·ªùng\n",
    "    df_processed['rating_count'] = df_processed['rating_count'].astype('Int64') \n",
    "\n",
    "print(\"ƒê√£ chuy·ªÉn c√°c c·ªôt sang ki·ªÉu d·ªØ li·ªáu s·ªë.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "0f5a7194",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# T√¨m d·ªØ li·ªáu b·ªã l·ªói trong c·ªôt 'rating' (tr√™n df g·ªëc v√¨ df_processed['rating'] ƒë√£ l√† numeric)\n",
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc0141a",
   "metadata": {},
   "source": [
    "Tra c·ª©u ƒëi·ªÉm ƒë√°nh gi√° c·ªßa s·∫£n ph·∫©m n√†y tr√™n Amazon b·∫±ng c√°ch t√¨m ki·∫øm product_id ƒë∆∞·ª£c cung c·∫•p tr√™n trang ch√≠nh th·ª©c c·ªßa h·ªç (amazon.in).\n",
    "\n",
    "ƒêi·ªÉm ƒë√°nh gi√° l√† 3.9. V√¨ v·∫≠y, ƒë√°nh gi√° s·∫£n ph·∫©m n√†y l√† 3.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "9d29398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th·ª±c hi·ªán tr√™n df_processed v√¨ df['rating'] c√≥ th·ªÉ v·∫´n l√† object\n",
    "df_processed['rating'] = df_processed['rating'].astype(str).str.replace('|', '3.9').astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "90800448",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# In th√¥ng tin t·ªïng quan c·ªßa DataFrame ƒë·ªÉ ki·ªÉm tra\n",
    "print(\"Th√¥ng tin DataFrame sau khi chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu:\")\n",
    "df_processed.info()\n",
    "print(df_processed.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d359a0d",
   "metadata": {},
   "source": [
    "## 3.2. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu vƒÉn b·∫£n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "ac097176",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "import nltk\n",
    "\n",
    "# T·∫£i c√°c g√≥i c·∫ßn thi·∫øt cho vi·ªác x·ª≠ l√Ω ng√¥n ng·ªØ (ch·∫°y 1 l·∫ßn)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')      # D√πng cho Tokenization\n",
    "    nltk.download('stopwords')  # Ch·ª©a danh s√°ch t·ª´ d·ª´ng\n",
    "    nltk.download('wordnet')    # D√πng cho Lemmatization\n",
    "    # nltk.download('punkt_tab') # C√≥ th·ªÉ kh√¥ng c·∫ßn thi·∫øt n·∫øu punkt ƒë√£ ƒë·ªß\n",
    "\n",
    "print(\"ƒê√£ t·∫£i/ki·ªÉm tra xong c√°c t√†i nguy√™n NLTK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1063e69",
   "metadata": {},
   "source": [
    "#### X·ª≠ l√Ω t√™n s·∫£n ph·∫©m - r√∫t g·ªçn v√† lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "512ddcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product_name(name):\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    name = name.replace('&', ' and ')\n",
    "    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
    "    # S·ª≠ d·ª•ng bi·ªÉu th·ª©c ch√≠nh quy (regex) ƒë·ªÉ lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh·ªèi chu·ªói name, gi·ªØ l·∫°i ch·ªâ ch·ªØ c√°i, s·ªë v√† kho·∫£ng tr·∫Øng.\n",
    "    cleaned_name = re.sub(r'[^\\w\\s]', ' ', name)\n",
    "    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
    "    cleaned_name = re.sub(r'\\s+', ' ', cleaned_name).strip()\n",
    "    # R√∫t g·ªçn t√™n s·∫£n ph·∫©m n·∫øu qu√° d√†i\n",
    "    return cleaned_name[:50] + '...' if len(cleaned_name) > 50 else cleaned_name\n",
    "# √Åp d·ª•ng h√†m l√†m s·∫°ch t√™n s·∫£n ph·∫©m\n",
    "df_processed['product_name'] = df_processed['product_name'].apply(clean_product_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d442ed7f",
   "metadata": {},
   "source": [
    "#### X·ª≠ l√Ω t√™n danh m·ª•c s·∫£n ph·∫©m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "4bda81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m danh m·ª•c ch√≠nh \n",
    "def extract_main_category(category):\n",
    "    if not isinstance(category, str):\n",
    "        return category\n",
    "    categories = category.split('|')\n",
    "    return categories[0] if categories else ''\n",
    "# Danh m·ª•c c·ª• th·ªÉ nh·∫•t (cu·ªëi c√πng)\n",
    "def extract_specific_category(category):\n",
    "    if not isinstance(category, str):\n",
    "        return category\n",
    "    categories = category.split('|')\n",
    "    return categories[-1] if categories else ''\n",
    "# √Åp d·ª•ng h√†m ƒë·ªÉ t·∫°o c·ªôt danh m·ª•c ch√≠nh v√† c·ª• th·ªÉ nh·∫•t\n",
    "df_processed['main_category'] = df_processed['category'].apply(extract_main_category)\n",
    "df_processed['specific_category'] = df_processed['category'].apply(extract_specific_category)\n",
    "# L√†m sach c·ªôt danh m·ª•c\n",
    "df_processed['main_category'] = df_processed['main_category'].apply(clean_product_name)\n",
    "df_processed['specific_category'] = df_processed['specific_category'].apply(clean_product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "12787463",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3767841",
   "metadata": {},
   "source": [
    "#### X·ª≠ l√Ω thu·ªôc t√≠nh text - r√∫t g·ªçn v√† lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "3e588252",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# In 5 d√≤ng ƒë·∫ßu c·ªßa c√°c thu·ªôc t√≠nh vƒÉn b·∫£n (tr∆∞·ªõc khi l√†m s·∫°ch s√¢u)\n",
    "text_columns_to_show = ['about_product', 'review_title', 'review_content']\n",
    "for col in text_columns_to_show:\n",
    "    print(f\"\\n5 d√≤ng ƒë·∫ßu c·ªßa c·ªôt {col} (TR∆Ø·ªöC L√ÄM S·∫†CH S√ÇU):\")\n",
    "    for i, val in enumerate(df_processed[col].head(5)):\n",
    "        print(f\"{i+1}. {str(val)[:200]}...\") # In 200 k√Ω t·ª± ƒë·∫ßu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "05b2badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'&+', ' and ', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'm\\s*media\\s*amazon\\s*com\\s*images\\S*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# √Åp d·ª•ng cho c√°c c·ªôt vƒÉn b·∫£n\n",
    "text_columns_to_clean = ['about_product', 'review_title', 'review_content']\n",
    "for col in text_columns_to_clean:\n",
    "    df_processed[col] = df_processed[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "be07c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text_nlp(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lo·∫°i b·ªè stop word v√† ch·ªâ gi·ªØ l·∫°i t·ª´ ch·ªØ c√°i\n",
    "    tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    # Gh√©p l·∫°i th√†nh chu·ªói\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "text_columns_to_preprocess = ['about_product', 'review_title', 'review_content']\n",
    "for col in text_columns_to_preprocess:\n",
    "    df_processed[col] = df_processed[col].apply(preprocess_text_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "f0cc887b",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# In 5 d√≤ng ƒë·∫ßu c·ªßa c√°c thu·ªôc t√≠nh vƒÉn b·∫£n (sau khi l√†m s·∫°ch s√¢u)\n",
    "for col in text_columns_to_show: # D√πng l·∫°i list t·ª´ cell tr∆∞·ªõc\n",
    "    print(f\"\\n5 d√≤ng ƒë·∫ßu c·ªßa c·ªôt {col} (SAU L√ÄM S·∫†CH S√ÇU):\")\n",
    "    for i, val in enumerate(df_processed[col].head(5)):\n",
    "        print(f\"{i+1}. {str(val)[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8758de7",
   "metadata": {},
   "source": [
    "## 3.3. L√†m s·∫°ch d·ªØ li·ªáu (X·ª≠ l√Ω thi·∫øu, tr√πng l·∫∑p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "2db6a2cd",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# X·ª≠ l√Ω gi√° tr·ªã thi·∫øu cho c·ªôt rating (n·∫øu c√≥)\n",
    "if df_processed['rating'].isnull().sum() > 0:\n",
    "    median_rating = df_processed['rating'].median()\n",
    "    df_processed['rating'].fillna(median_rating, inplace=True)\n",
    "    print(f\"ƒê√£ l·∫•p ƒë·∫ßy gi√° tr·ªã thi·∫øu trong 'rating' b·∫±ng trung v·ªã: {median_rating}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu trong 'rating' ƒë·ªÉ l·∫•p.\")\n",
    "\n",
    "# X·ª≠ l√Ω gi√° tr·ªã thi·∫øu cho rating_count (n·∫øu c√≥)\n",
    "if df_processed['rating_count'].isnull().sum() > 0:\n",
    "    median_rating_count = df_processed['rating_count'].median()\n",
    "    df_processed['rating_count'].fillna(median_rating_count, inplace=True)\n",
    "    # ƒê·∫£m b·∫£o ki·ªÉu Int64 sau khi fillna\n",
    "    df_processed['rating_count'] = df_processed['rating_count'].astype('Int64')\n",
    "    print(f\"ƒê√£ l·∫•p ƒë·∫ßy gi√° tr·ªã thi·∫øu trong 'rating_count' b·∫±ng trung v·ªã: {median_rating_count}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu trong 'rating_count' ƒë·ªÉ l·∫•p.\")\n",
    "\n",
    "print(\"\\nKi·ªÉm tra l·∫°i gi√° tr·ªã thi·∫øu sau x·ª≠ l√Ω:\")\n",
    "print(df_processed.isnull().sum()[df_processed.isnull().sum() > 0])\n",
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "d47cedbd",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# X·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p\n",
    "print(f\"S·ªë d√≤ng tr√πng l·∫∑p tr∆∞·ªõc khi lo·∫°i b·ªè: {df_processed.duplicated().sum()}\")\n",
    "df_processed.drop_duplicates(inplace=True)\n",
    "print(f\"S·ªë d√≤ng tr√πng l·∫∑p sau khi lo·∫°i b·ªè: {df_processed.duplicated().sum()}\")\n",
    "print(f\"K√≠ch th∆∞·ªõc d·ªØ li·ªáu sau khi lo·∫°i b·ªè tr√πng l·∫∑p: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e9a2a",
   "metadata": {},
   "source": [
    "Kh√¥ng c√≥ d·ªØ li·ªáu tr√πng l·∫∑p "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f557fa9",
   "metadata": {},
   "source": [
    "# 4. KH√ÅM PH√Å V√Ä PH√ÇN T√çCH D·ªÆ LI·ªÜU (EDA) (Ti·∫øp theo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c0c45",
   "metadata": {},
   "source": [
    "## 4.1. Th·ªëng k√™, ph√¢n ph·ªëi d·ªØ li·ªáu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "f9661590",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "df_processed[['discounted_price', 'actual_price', 'discount_percentage', 'rating', 'rating_count']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "087397aa",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# Tr·ª±c quan h√≥a ph√¢n ph·ªëi c·ªßa c√°c c·ªôt s·ªë \n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.histplot(df_processed['discounted_price'].dropna(), bins=30, kde=True, color='orange')\n",
    "plt.title('Ph√¢n ph·ªëi gi√° ƒë√£ gi·∫£m (Discounted Price)')\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.histplot(df_processed['actual_price'].dropna(), bins=30, kde=True, color='green')\n",
    "plt.title('Ph√¢n ph·ªëi gi√° g·ªëc (Actual Price)')\n",
    "plt.subplot(3,2,3)\n",
    "sns.histplot(df_processed['discount_percentage'].dropna(), bins=30, kde=True, color='blue')\n",
    "plt.title('Ph√¢n ph·ªëi ph·∫ßn trƒÉm gi·∫£m gi√° (Discount Percentage)')\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.histplot(df_processed['rating'].dropna(), bins=30, kde=True, color='purple')\n",
    "plt.title('Ph√¢n ph·ªëi ƒë√°nh gi√° (Rating)')\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.histplot(df_processed['rating_count'].dropna(), bins=30, kde=True, color='red')\n",
    "plt.title('Ph√¢n ph·ªëi s·ªë l∆∞·ª£ng ƒë√°nh gi√° (Rating Count)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b149a",
   "metadata": {},
   "source": [
    "**C√°c thu·ªôc t√≠nh s·ªë ƒë·ªÅu c√≥ ph√¢n ph·ªëi kh√¥ng chu·∫©n:**\n",
    "- Discounted_price v√† actual_price c√≥ ph√¢n ph·ªëi l·ªách ph·∫£i: cho th·∫•y gi√° g·ªëc v√† gi√° ƒë√£ gi·∫£m n·∫±m ·ªü m·ª©c th·∫•p, nhi·ªÅu nh·∫•t ·ªü kho·∫£ng d∆∞·ªõi 2000 Rupee (~$23/600.000VND).\n",
    "- Ph·∫ßn trƒÉm gi·∫£m gi√° (Discount percentage) ph√¢n ph·ªëi tr·∫£i r·ªông, t·∫≠p trung nhi·ªÅu nh·∫•t ·ªü kho·∫£ng 40-60% cho m·ªôt s·∫£n ph·∫©m.\n",
    "- Rating l·∫°i c√≥ ph√¢n ph·ªëi l·ªách tr√°i: cho th·∫•y ƒëi·ªÉm ƒë√°nh gi√° cho c√°c s·∫£n ph·∫©m kh√° cao, ch·ªß y·∫øu ·ªü m·ª©c 3.5 - 4.5. \n",
    "- S·ªë ƒë√°nh gi√° cho m·ªói s·∫£n ph·∫©m c√≥ ph√¢n ph·ªëi l·ªách ph·∫£i: kho·∫£ng d∆∞·ªõi 100.000 ƒë√°nh gi√° cho m·ªôt s·∫£n ph·∫©m, m·ªôt s·ªë √≠t c√≥ s·ªë l∆∞·ª£ng ƒë√°nh gi√° cao.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "5c096b5a",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "df_processed.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57255981",
   "metadata": {},
   "source": [
    "- Danh m·ª•c s·∫£n ph·∫©m ch√≠nh xu·∫•t hi·ªán nhi·ªÅu nh·∫•t l√† Electronics v√† lo·∫°i USBCables. \n",
    "- C√°c danh m·ª•c s·∫£n ph·∫©m duy nh·∫•t: 9\n",
    "- C√°c danh m·ª•c c·ª• th·ªÉ nh·∫•t duy nh·∫•t: 207\n",
    "- C√≥ 1290 lo·∫°i s·∫£n ph·∫©m kh√°c nhau "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f4a4af",
   "metadata": {},
   "source": [
    "## 4.2. Grouping and Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bc85d",
   "metadata": {},
   "source": [
    "### ƒê√°nh gi√° trung b√¨nh c·ªßa c√°c danh m·ª•c s·∫£n ph·∫©m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "592b3a7c",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# Nh√≥m theo danh m·ª•c ch√≠nh v√† t√≠nh trung b√¨nh c·ªßa c·ªôt 'rating'\n",
    "grouped_df = df_processed.groupby('main_category')['rating'].mean()\n",
    "print(\"\\nTrung b√¨nh ƒë√°nh gi√° theo danh m·ª•c ch√≠nh:\")\n",
    "print(grouped_df.sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6714c6",
   "metadata": {},
   "source": [
    "- C√°c ƒë√°nh gi√° cao nh·∫•t thu·ªôc v·ªÅ danh m·ª•c s·∫£n ph·∫©m vƒÉn ph√≤ng ph·∫©m, ƒë·ªì ch∆°i v√† games.\n",
    "- ƒê·ªì ƒëi·ªán t·ª≠, danh m·ª•c s·∫£n ph·∫©m cho nh√† b·∫øp, s·ª©c kh·ªèe ƒë√°nh gi√° n·∫±m ·ªü m·ª©c trung b√¨nh.\n",
    "- Nh·∫°c c·ª• v√† c√°c lo·∫°i xe mua qua s√†n th∆∞∆°ng m·∫°i Amazon l·∫°i c√≥ ƒë√°nh gi√° trung b√¨nh th·∫•p. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92193d",
   "metadata": {},
   "source": [
    "### Top 10 s·∫£n ph·∫©m c√≥ s·ªë l∆∞·ª£ng ƒë√°nh gi√° nhi·ªÅu nh·∫•t ·ªü c√°c danh m·ª•c s·∫£n ph·∫©m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "4248ba4f",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "product_gr_cate_rating_count = df_processed.groupby(['main_category', 'product_name'])['rating_count'].sum().reset_index()\n",
    "product_gr_cate_rating_count = product_gr_cate_rating_count.sort_values(by='rating_count', ascending=False)\n",
    "print(\"\\nS·ªë l∆∞·ª£ng ƒë√°nh gi√° theo danh m·ª•c ch√≠nh v√† t√™n s·∫£n ph·∫©m:\")\n",
    "product_gr_cate_rating_count.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "b6363d17",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# Tr·ª±c quan h√≥a s·ªë l∆∞·ª£ng ƒë√°nh gi√° theo danh m·ª•c ch√≠nh\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='rating_count', y='main_category', data=product_gr_cate_rating_count.head(10), palette='Set1', hue='product_name')\n",
    "plt.title('S·ªë l∆∞·ª£ng ƒë√°nh gi√° theo danh m·ª•c ch√≠nh')\n",
    "plt.xlabel('S·ªë l∆∞·ª£ng ƒë√°nh gi√°')\n",
    "plt.ylabel('Danh m·ª•c ch√≠nh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955acdb9",
   "metadata": {},
   "source": [
    "- Danh m·ª•c c√≥ s·ªë l∆∞·ª£ng s·∫£n ph·∫©m ƒë∆∞·ª£c ƒë√°nh gi√° nhi·ªÅu nh·∫•t l√† Electronics, v√† ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh cho danh m·ª•c n√†y n·∫±m ·ªü m·ª©c 4.0 (Kh√°). ƒê·ª©ng ƒë·∫ßu l√† C√°p AmazonBasics. \n",
    "- Danh m·ª•c c√≥ s·∫£n ph·∫©m b√°n ch·∫°y n·∫±m trong Top 10 l√† Computers and Accessories v·ªõi s·∫£n ph·∫©m Deuce USB. V√† Computers and Accessories c≈©ng c√≥ rating trung b√¨nh l√† 4.15 n·∫±m ·ªü m·ª©c kh√°. <br>\n",
    "**--> Nh·ªØng s·∫£n ph·∫©m c√≥ s·ªë l∆∞·ª£ng ƒë√°nh gi√° cao nh·∫•t trong t·ª´ng danh m·ª•c c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† c√°c m·∫∑t h√†ng b√°n ch·∫°y ti·ªÅm nƒÉng, ngay c·∫£ khi kh√¥ng c√≥ d·ªØ li·ªáu b√°n h√†ng tr·ª±c ti·∫øp.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab49acc",
   "metadata": {},
   "source": [
    "# 5. X·ª¨ L√ù OUTLIERS, NHI·ªÑU (cho ph√¢n c·ª•m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "d6d96844",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "numeric_cols_for_clustering = ['discounted_price', 'actual_price', 'discount_percentage', 'rating', 'rating_count']\n",
    "\n",
    "# V·∫Ω boxplot ƒë·ªÉ ki·ªÉm tra outliers\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i, col in enumerate(numeric_cols_for_clustering, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    # Ki·ªÉm tra xem c·ªôt c√≥ t·ªìn t·∫°i v√† c√≥ d·ªØ li·ªáu kh√¥ng tr∆∞·ªõc khi v·∫Ω\n",
    "    if col in df_processed.columns and not df_processed[col].dropna().empty:\n",
    "        sns.boxplot(x=df_processed[col])\n",
    "        plt.title(col)\n",
    "    else:\n",
    "        plt.title(f\"{col} (No data or all NaN)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "8443c4b6",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# Ki·ªÉm tra t·ª∑ l·ªá outliers c·ªßa t·ª´ng bi·∫øn s·ªë b·∫±ng ph∆∞∆°ng ph√°p IQR\n",
    "def compute_iqr_bounds(series):\n",
    "    # B·ªè qua NaN khi t√≠nh quantile\n",
    "    series_cleaned = series.dropna()\n",
    "    if series_cleaned.empty:\n",
    "        return np.nan, np.nan # Tr·∫£ v·ªÅ NaN n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu\n",
    "    Q1 = series_cleaned.quantile(0.25)\n",
    "    Q3 = series_cleaned.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return lower, upper\n",
    "\n",
    "def outlier_ratio_iqr(series):\n",
    "    series_cleaned = series.dropna()\n",
    "    if series_cleaned.empty:\n",
    "        return 0, 0.0\n",
    "    lower, upper = compute_iqr_bounds(series_cleaned)\n",
    "    outliers = ((series_cleaned < lower) | (series_cleaned > upper)).sum()\n",
    "    ratio = outliers / series_cleaned.shape[0]\n",
    "    return outliers, ratio\n",
    "\n",
    "print(\"T·ª∑ l·ªá outliers theo t·ª´ng bi·∫øn s·ªë (cho ph√¢n c·ª•m):\")\n",
    "for col in numeric_cols_for_clustering:\n",
    "    if col in df_processed.columns:\n",
    "        outliers, ratio = outlier_ratio_iqr(df_processed[col])\n",
    "        print(f\"{col}: {outliers} outliers ({ratio:.2%})\")\n",
    "    else:\n",
    "        print(f\"{col}: column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "05134f6b",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "# df_clustering l√† b·∫£n sao c·ªßa df_processed ƒë·ªÉ x·ª≠ l√Ω outliers ri√™ng cho ph√¢n c·ª•m\n",
    "df_clustering = df_processed.copy()\n",
    "\n",
    "def remove_outliers_by_quantile(df_in, column, lower_quantile=0.05, upper_quantile=0.95):\n",
    "    # B·ªè qua NaN khi t√≠nh quantile\n",
    "    series_cleaned = df_in[column].dropna()\n",
    "    if series_cleaned.empty:\n",
    "        return df_in # Tr·∫£ v·ªÅ df g·ªëc n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu\n",
    "    \n",
    "    lower_bound = series_cleaned.quantile(lower_quantile)\n",
    "    upper_bound = series_cleaned.quantile(upper_quantile)\n",
    "    return df_in[(df_in[column] >= lower_bound) & (df_in[column] <= upper_bound)]\n",
    "\n",
    "# Ch·ªâ x·ª≠ l√Ω outliers cho 'discounted_price' cho m·ª•c ƒë√≠ch ph√¢n c·ª•m nh∆∞ trong notebook g·ªëc\n",
    "col_to_cap = 'discounted_price'\n",
    "if col_to_cap in df_clustering.columns:\n",
    "    before = df_clustering.shape[0]\n",
    "    df_clustering = remove_outliers_by_quantile(df_clustering, col_to_cap)\n",
    "    after = df_clustering.shape[0]\n",
    "    print(f\" ƒê√£ lo·∫°i b·ªè {(before - after)} d√≤ng ngo√†i kho·∫£ng 90% trung t√¢m ·ªü c·ªôt '{col_to_cap}' cho df_clustering.\")\n",
    "else:\n",
    "    print(f\"C·ªôt {col_to_cap} kh√¥ng t·ªìn t·∫°i trong df_clustering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c8488",
   "metadata": {},
   "source": [
    "# 6. Chu·∫©n h√≥a d·ªØ li·ªáu (cho ph√¢n c·ª•m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f22d5",
   "metadata": {},
   "source": [
    "#### Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë (cho ph√¢n c·ª•m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "8034ce98",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols_cluster = ['discounted_price', 'actual_price', 'rating'] # C√°c c·ªôt d√πng cho ph√¢n c·ª•m\n",
    "scaler_cluster = StandardScaler()\n",
    "\n",
    "# ƒê·∫£m b·∫£o c√°c c·ªôt n√†y t·ªìn t·∫°i v√† kh√¥ng c√≥ NaN tr∆∞·ªõc khi scale\n",
    "df_clustering.dropna(subset=num_cols_cluster, inplace=True)\n",
    "\n",
    "if not df_clustering.empty:\n",
    "    df_clustering[num_cols_cluster] = scaler_cluster.fit_transform(df_clustering[num_cols_cluster])\n",
    "    print(\"ƒê√£ chu·∫©n h√≥a c√°c c·ªôt s·ªë cho ph√¢n c·ª•m:\", num_cols_cluster)\n",
    "    print(df_clustering[num_cols_cluster].head())\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ chu·∫©n h√≥a cho ph√¢n c·ª•m sau khi lo·∫°i b·ªè NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4f151",
   "metadata": {},
   "source": [
    "#### Chu·∫©n h√≥a vƒÉn b·∫£n (cho ph√¢n c·ª•m - about_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "9e723451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers # N·∫øu ch∆∞a c√†i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "27a711a1",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# TF-IDF v·ªõi 1000 chi·ªÅu cho df_clustering['about_product']\n",
    "if 'about_product' in df_clustering.columns and not df_clustering['about_product'].dropna().empty:\n",
    "    tfidf_cluster = TfidfVectorizer(max_features=1000)\n",
    "    # Fillna cho about_product n·∫øu c√≥ gi√° tr·ªã thi·∫øu tr∆∞·ªõc khi fit_transform\n",
    "    about_product_cleaned = df_clustering['about_product'].fillna('') \n",
    "    about_embeddings_cluster = tfidf_cluster.fit_transform(about_product_cleaned)\n",
    "    print(\"ƒê√£ ho√†n th√†nh vi·ªác hu·∫•n luy·ªán TfidfVectorizer cho ph√¢n c·ª•m.\")\n",
    "    total_unique_words_cluster = len(tfidf_cluster.vocabulary_)\n",
    "    print(f\"T·ªïng s·ªë t·ª´ ƒë·ªôc l·∫≠p trong about_product (cho ph√¢n c·ª•m) l√†: {total_unique_words_cluster}\")\n",
    "else:\n",
    "    print(\"C·ªôt 'about_product' kh√¥ng c√≥ d·ªØ li·ªáu ho·∫∑c to√†n NaN trong df_clustering.\")\n",
    "    about_embeddings_cluster = None # ƒê·ªÉ tr√°nh l·ªói ·ªü cell sau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "c630d13e",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "if about_embeddings_cluster is not None:\n",
    "    svd_cluster = TruncatedSVD(n_components=300, random_state=42) \n",
    "    about_embeddings_reduced_cluster = svd_cluster.fit_transform(about_embeddings_cluster)\n",
    "    print(\"Variance explained (cho ph√¢n c·ª•m):\", svd_cluster.explained_variance_ratio_.sum())\n",
    "else:\n",
    "    about_embeddings_reduced_cluster = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd596e8",
   "metadata": {},
   "source": [
    "#### H·ª£p c√°c thu·ªôc t√≠nh l·∫°i (cho ph√¢n c·ª•m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "473d5f0f",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ch·ªâ th·ª±c hi·ªán n·∫øu about_embeddings_reduced_cluster ƒë√£ ƒë∆∞·ª£c t·∫°o\n",
    "if about_embeddings_reduced_cluster is not None and not df_clustering.empty:\n",
    "    # L·∫•y 2 c·ªôt s·ªë ƒë√£ chu·∫©n h√≥a t·ª´ df_clustering\n",
    "    X_num_cluster = df_clustering[['discounted_price', 'rating']].values\n",
    "\n",
    "    # Nh√¢n tr·ªçng s·ªë cho ƒë·∫∑c tr∆∞ng s·ªë \n",
    "    weight = 10\n",
    "    X_num_weighted_cluster = X_num_cluster * weight\n",
    "\n",
    "    # Gh√©p v·ªõi embedding\n",
    "    # ƒê·∫£m b·∫£o s·ªë d√≤ng kh·ªõp nhau (n·∫øu c√≥ dropna tr∆∞·ªõc ƒë√≥)\n",
    "    if X_num_weighted_cluster.shape[0] == about_embeddings_reduced_cluster.shape[0]:\n",
    "        X_cluster = np.hstack([X_num_weighted_cluster, about_embeddings_reduced_cluster])\n",
    "        print(\"Shape ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o cho ph√¢n c·ª•m:\", X_cluster.shape)\n",
    "    else:\n",
    "        print(\"L·ªói: S·ªë d√≤ng kh√¥ng kh·ªõp gi·ªØa features s·ªë v√† features vƒÉn b·∫£n cho ph√¢n c·ª•m.\")\n",
    "        print(f\"X_num_weighted_cluster shape: {X_num_weighted_cluster.shape}\")\n",
    "        print(f\"about_embeddings_reduced_cluster shape: {about_embeddings_reduced_cluster.shape}\")\n",
    "        X_cluster = None\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ t·∫°o X_cluster do thi·∫øu features.\")\n",
    "    X_cluster = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf7802",
   "metadata": {},
   "source": [
    "# 7. Ph√¢n c·ª•m theo Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "b417da5c",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "if X_cluster is not None:\n",
    "    inertia = []\n",
    "    silhouette = []\n",
    "    K_cluster = range(2, 11)\n",
    "\n",
    "    for k in K_cluster:\n",
    "        kmeans_model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels_km = kmeans_model.fit_predict(X_cluster)\n",
    "        inertia.append(kmeans_model.inertia_)\n",
    "        if len(np.unique(labels_km)) > 1: # C·∫ßn √≠t nh·∫•t 2 c·ª•m ƒë·ªÉ t√≠nh silhouette\n",
    "            silhouette.append(silhouette_score(X_cluster, labels_km))\n",
    "        else:\n",
    "            silhouette.append(np.nan) # Ho·∫∑c gi√° tr·ªã kh√°c ƒë·ªÉ b√°o l·ªói\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(K_cluster, inertia, marker='o')\n",
    "    plt.xlabel('S·ªë c·ª•m (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method for K-Means (Clustering)')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(K_cluster, silhouette, marker='o', color='orange')\n",
    "    plt.xlabel('S·ªë c·ª•m (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score for K-Means (Clustering)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ th·ª±c hi·ªán K-Means do X_cluster kh√¥ng ƒë∆∞·ª£c t·∫°o.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "d0a1c259",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "if X_cluster is not None and not df_clustering.empty:\n",
    "    # Ph√¢n c·ª•m KMeans v·ªõi k=3 (v√≠ d·ª•, d·ª±a tr√™n elbow/silhouette)\n",
    "    kmeans_final = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "    labels_kmeans_final = kmeans_final.fit_predict(X_cluster)\n",
    "\n",
    "    # Th√™m nh√£n c·ª•m v√†o df_clustering (ƒë·∫£m b·∫£o index kh·ªõp n·∫øu c√≥ dropna)\n",
    "    # N·∫øu X_cluster ƒë∆∞·ª£c t·∫°o t·ª´ df_clustering sau khi dropna, index c·ªßa df_clustering c·∫ßn ƒë∆∞·ª£c reset\n",
    "    # ho·∫∑c d√πng index g·ªëc n·∫øu X_cluster gi·ªØ ƒë∆∞·ª£c index ƒë√≥.\n",
    "    # Hi·ªán t·∫°i, X_num_cluster v√† about_embeddings_reduced_cluster kh√¥ng gi·ªØ index g·ªëc.\n",
    "    # C√°ch ƒë∆°n gi·∫£n l√† g√°n l·∫°i cho df_clustering n·∫øu s·ªë d√≤ng kh·ªõp.\n",
    "    if len(labels_kmeans_final) == df_clustering.shape[0]:\n",
    "        df_clustering['kmeans_cluster'] = labels_kmeans_final\n",
    "        # Th·ªëng k√™ s·ªë l∆∞·ª£ng ƒëi·ªÉm trong m·ªói c·ª•m\n",
    "        print(\"S·ªë l∆∞·ª£ng ƒëi·ªÉm trong m·ªói c·ª•m K-Means:\")\n",
    "        print(df_clustering['kmeans_cluster'].value_counts())\n",
    "    else:\n",
    "        print(\"L·ªói: S·ªë d√≤ng c·ªßa nh√£n K-Means kh√¥ng kh·ªõp v·ªõi df_clustering.\")\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ g√°n nh√£n K-Means do X_cluster ho·∫∑c df_clustering kh√¥ng h·ª£p l·ªá.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "f87dba12",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "if X_cluster is not None and 'kmeans_cluster' in df_clustering.columns:\n",
    "    # Gi·∫£m chi·ªÅu d·ªØ li·ªáu xu·ªëng 2 th√†nh ph·∫ßn ch√≠nh v·ªõi PCA\n",
    "    pca_cluster_viz = PCA(n_components=2, random_state=42)\n",
    "    X_pca_cluster = pca_cluster_viz.fit_transform(X_cluster)\n",
    "\n",
    "    # V·∫Ω scatter plot, m·ªói c·ª•m m·ªôt m√†u\n",
    "    plt.figure(figsize=(8,6))\n",
    "    scatter_km = plt.scatter(X_pca_cluster[:,0], X_pca_cluster[:,1], c=df_clustering['kmeans_cluster'], cmap='Set1', alpha=0.7)\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('KMeans Clusters Visualization (PCA 2D) for Clustering')\n",
    "    plt.legend(*scatter_km.legend_elements(), title=\"Cluster\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ tr·ª±c quan h√≥a K-Means.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "96b28ad5",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "if X_cluster is not None and 'kmeans_cluster' in df_clustering.columns:\n",
    "    sil_score_km = silhouette_score(X_cluster, df_clustering['kmeans_cluster'])\n",
    "    db_score_km = davies_bouldin_score(X_cluster, df_clustering['kmeans_cluster'])\n",
    "    print(f\"Silhouette Score (K-Means): {sil_score_km:.3f}\")\n",
    "    print(f\"Davies-Bouldin Index (K-Means): {db_score_km:.3f}\")\n",
    "\n",
    "    # Th·ªëng k√™ ƒë·∫∑c tr∆∞ng t·ª´ng c·ª•m K-Means\n",
    "    print(df_clustering.groupby('kmeans_cluster')[['discounted_price', 'rating']].mean())\n",
    "    print(df_clustering['kmeans_cluster'].value_counts())\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ ƒë√°nh gi√° K-Means.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7323f",
   "metadata": {},
   "source": [
    "G√°n nh√£n cho c√°c c·ª•m KMeans:\n",
    "\n",
    "C·ª•m 0: \"S·∫£n ph·∫©m gi√° th·∫•p, ch·∫•t l∆∞·ª£ng k√©m\" (336 s·∫£n ph·∫©m - 24%).\n",
    "ƒê·∫∑c ƒëi·ªÉm: Gi√° th·∫•p h∆°n trung b√¨nh, ƒë√°nh gi√° r·∫•t th·∫•p.\n",
    "√ù nghƒ©a: S·∫£n ph·∫©m gi√° r·∫ª nh∆∞ng ch·∫•t l∆∞·ª£ng kh√¥ng ƒë∆∞·ª£c ƒë√°nh gi√° cao.\n",
    "ƒê√°p ·ª©ng c√¢u h·ªèi: Ph√°t hi·ªán s·∫£n ph·∫©m ƒë∆∞·ª£c \"ƒë·ªãnh gi√° kh√¥ng h·ª£p l√Ω\" - gi√° c√≥ th·ªÉ ph√π h·ª£p nh∆∞ng ch·∫•t l∆∞·ª£ng k√©m.\n",
    "\n",
    "C·ª•m 1: \"S·∫£n ph·∫©m gi√° tr·ªã t·ªët\" (931 s·∫£n ph·∫©m - 67%).\n",
    "ƒê·∫∑c ƒëi·ªÉm: Gi√° th·∫•p h∆°n trung b√¨nh, ƒë√°nh gi√° cao h∆°n trung b√¨nh.\n",
    "√ù nghƒ©a: ƒê√¢y l√† ph√¢n kh√∫c \"value for money\" - gi√° c·∫£ ph·∫£i chƒÉng nh∆∞ng ch·∫•t l∆∞·ª£ng t·ªët.\n",
    "ƒê√°p ·ª©ng c√¢u h·ªèi: X√°c ƒë·ªãnh ƒë∆∞·ª£c nh√≥m s·∫£n ph·∫©m mang l·∫°i \"gi√° tr·ªã t·ªët nh·∫•t\" v√† c√≥ th·ªÉ l√† \"m√≥n h·ªùi ti·ªÅm ·∫©n\".\n",
    "\n",
    "C·ª•m 2: \"S·∫£n ph·∫©m cao c·∫•p\" (127 s·∫£n ph·∫©m - 9%).\n",
    "ƒê·∫∑c ƒëi·ªÉm: Gi√° cao h∆°n trung b√¨nh r·∫•t nhi·ªÅu, ƒë√°nh gi√° cao h∆°n trung b√¨nh.\n",
    "√ù nghƒ©a: S·∫£n ph·∫©m cao c·∫•p, gi√° cao nh∆∞ng ch·∫•t l∆∞·ª£ng ƒë∆∞·ª£c ƒë√°nh gi√° t·ªët.\n",
    "ƒê√°p ·ª©ng c√¢u h·ªèi: Ph√¢n bi·ªát ƒë∆∞·ª£c nh√≥m s·∫£n ph·∫©m \"cao c·∫•p\" v·ªõi c√°c ph√¢n kh√∫c kh√°c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85f6ba",
   "metadata": {},
   "source": [
    "# 8. Ph√¢n c·ª•m ph√¢n c·∫•p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "4b45c811",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "if X_cluster is not None:\n",
    "    # L·∫•y m·∫´u ng·∫´u nhi√™n (n·∫øu d·ªØ li·ªáu qu√° l·ªõn)\n",
    "    sample_size_hc = min(500, X_cluster.shape[0])\n",
    "    indices_hc = np.random.choice(X_cluster.shape[0], sample_size_hc, replace=False)\n",
    "    X_sample_hc = X_cluster[indices_hc]\n",
    "\n",
    "    linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "    affinity_metrics = ['euclidean', 'cosine']\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=len(linkage_methods), \n",
    "                             ncols=len(affinity_metrics), \n",
    "                             figsize=(15, 12), \n",
    "                             sharex=False, sharey=False)\n",
    "    fig.tight_layout(pad=4.0)\n",
    "\n",
    "    for i, method in enumerate(linkage_methods):\n",
    "        for j, metric in enumerate(affinity_metrics):\n",
    "            ax = axes[i, j]\n",
    "            if method == 'ward' and metric != 'euclidean':\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f\"{method} + {metric}\\n(SKIP: ward ch·ªâ h·ªó tr·ª£ euclidean)\", fontsize=9)\n",
    "                continue\n",
    "            try:\n",
    "                Z = linkage(X_sample_hc, method=method, metric=metric)\n",
    "            except Exception as e:\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f\"{method} + {metric}\\n(L·ªói: {e})\", fontsize=9)\n",
    "                continue\n",
    "            dendrogram(Z, ax=ax, orientation='top', distance_sort='descending', \n",
    "                       show_leaf_counts=False, truncate_mode='lastp', p=30)\n",
    "            ax.set_title(f\"{method} + {metric}\", fontsize=10)\n",
    "            ax.set_ylabel(\"Kho·∫£ng c√°ch\")\n",
    "            ax.set_xticks([])\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.6, wspace=0.4)\n",
    "    plt.suptitle(\"So s√°nh Dendrogram cho c√°c T·ªï h·ª£p linkage‚Äìmetric (Clustering)\", y=1.02, fontsize=14)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ v·∫Ω dendrogram do X_cluster kh√¥ng ƒë∆∞·ª£c t·∫°o.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "33fba45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_method = 'ward'\n",
    "best_affinity = 'euclidean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "c4afe8e9",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "if X_cluster is not None and not df_clustering.empty:\n",
    "    # Ph√¢n c·ª•m ph√¢n c·∫•p v·ªõi ph∆∞∆°ng ph√°p t·ªët nh·∫•t\n",
    "    agglo = AgglomerativeClustering(n_clusters=3, metric=best_affinity, linkage=best_method)\n",
    "    agglo_labels = agglo.fit_predict(X_cluster)\n",
    "    \n",
    "    if len(agglo_labels) == df_clustering.shape[0]:\n",
    "        df_clustering['agglo_cluster'] = agglo_labels\n",
    "    else:\n",
    "        print(\"L·ªói: S·ªë d√≤ng c·ªßa nh√£n Agglomerative kh√¥ng kh·ªõp v·ªõi df_clustering.\")\n",
    "        if 'agglo_cluster' in df_clustering.columns: df_clustering.drop('agglo_cluster', axis=1, inplace=True) # X√≥a c·ªôt n·∫øu c√≥ l·ªói\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ th·ª±c hi·ªán Agglomerative Clustering do X_cluster ho·∫∑c df_clustering kh√¥ng h·ª£p l·ªá.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "b1615c16",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "if X_cluster is not None and 'agglo_cluster' in df_clustering.columns:\n",
    "    # Tr·ª±c quan h√≥a k·∫øt qu·∫£ ph√¢n c·ª•m ph√¢n c·∫•p (X_pca_cluster ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü ph·∫ßn K-Means)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter_agglo = plt.scatter(X_pca_cluster[:,0], X_pca_cluster[:,1], c=df_clustering['agglo_cluster'], cmap='viridis', alpha=0.7)\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title(f'Agglomerative Clusters (method={best_method}, n_clusters=3) for Clustering')\n",
    "    plt.colorbar(scatter_agglo, label='Cluster')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ tr·ª±c quan h√≥a Agglomerative Clustering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null, # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "id": "1863dd54",
   "metadata": {},
   "outputs": [], # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi ch·∫°y
   "source": [
    "if X_cluster is not None and 'agglo_cluster' in df_clustering.columns:\n",
    "    # ƒê√°nh gi√° k·∫øt qu·∫£ ph√¢n c·ª•m\n",
    "    sil_score_agglo = silhouette_score(X_cluster, df_clustering['agglo_cluster'])\n",
    "    db_score_agglo = davies_bouldin_score(X_cluster, df_clustering['agglo_cluster'])\n",
    "    print(f\"Silhouette Score (Agglomerative - {best_method}): {sil_score_agglo:.3f}\")\n",
    "    print(f\"Davies-Bouldin Index (Agglomerative - {best_method}): {db_score_agglo:.3f}\")\n",
    "\n",
    "    # Th·ªëng k√™ ƒë·∫∑c tr∆∞ng t·ª´ng c·ª•m Agglomerative\n",
    "    print(\"\\nTh·ªëng k√™ ƒë·∫∑c tr∆∞ng t·ª´ng c·ª•m Agglomerative:\")\n",
    "    print(df_clustering.groupby('agglo_cluster')[['discounted_price', 'rating']].mean())\n",
    "    print(df_clustering['agglo_cluster'].value_counts())\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ ƒë√°nh gi√° Agglomerative Clustering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b657db",
   "metadata": {},
   "source": [
    "Di·ªÖn gi·∫£i v√† g√°n nh√£n cho c√°c c·ª•m (Agglomerative):\n",
    "\n",
    "C·ª•m 0: \"S·∫£n ph·∫©m ƒë·∫°i tr√†, ch·∫•t l∆∞·ª£ng trung b√¨nh-th·∫•p\" (738 s·∫£n ph·∫©m - 53%)\n",
    "ƒê·∫∑c ƒëi·ªÉm: Gi√° th·∫•p h∆°n trung b√¨nh, ƒë√°nh gi√° th·∫•p h∆°n trung b√¨nh\n",
    "Kh√°c bi·ªát v·ªõi KMeans: ƒê√¢y l√† c·ª•m l·ªõn nh·∫•t, nh∆∞ng rating kh√¥ng qu√° th·∫•p nh∆∞ c·ª•m 0 c·ªßa KMeans\n",
    "√ù nghƒ©a: S·∫£n ph·∫©m ph·ªï th√¥ng, gi√° r·∫ª, ch·∫•t l∆∞·ª£ng t∆∞∆°ng ƒë·ªëi ph√π h·ª£p v·ªõi gi√° ti·ªÅn\n",
    "\n",
    "C·ª•m 1: \"S·∫£n ph·∫©m cao c·∫•p\" (124 s·∫£n ph·∫©m - 9%)\n",
    "ƒê·∫∑c ƒëi·ªÉm: Gi√° cao h∆°n trung b√¨nh r·∫•t nhi·ªÅu (~2.8 ƒë·ªô l·ªách chu·∫©n), ƒë√°nh gi√° ch·ªâ nh·ªânh h∆°n trung b√¨nh m·ªôt ch√∫t\n",
    "Kh√°c bi·ªát v·ªõi KMeans: G·∫ßn nh∆∞ gi·ªëng h·ªát c·ª•m 2 c·ªßa KMeans\n",
    "√ù nghƒ©a: S·∫£n ph·∫©m cao c·∫•p nh∆∞ng c√≥ v·∫ª b·ªã ƒë·ªãnh gi√° cao so v·ªõi ch·∫•t l∆∞·ª£ng th·ª±c t·∫ø c·∫£m nh·∫≠n\n",
    "\n",
    "C·ª•m 2: \"S·∫£n ph·∫©m gi√° tr·ªã t·ªët nh·∫•t\" (532 s·∫£n ph·∫©m - 38%)\n",
    "ƒê·∫∑c ƒëi·ªÉm: Gi√° th·∫•p h∆°n trung b√¨nh, nh∆∞ng rating cao h∆°n trung b√¨nh r·∫•t nhi·ªÅu\n",
    "Kh√°c bi·ªát v·ªõi KMeans: Rating cao h∆°n h·∫≥n (0.85 so v·ªõi 0.44)\n",
    "√ù nghƒ©a: ƒê√¢y l√† nh·ªØng s·∫£n ph·∫©m \"m√≥n h·ªùi\" th·ª±c s·ª± - gi√° th·∫•p nh∆∞ng ch·∫•t l∆∞·ª£ng ƒë∆∞·ª£c ƒë√°nh gi√° r·∫•t cao"
   ]
  },
  # --- M·ªöI TH√äM V√ÄO: B·∫Øt ƒë·∫ßu ph·∫ßn m√¥ h√¨nh H·ªìi quy v√† Ph√¢n lo·∫°i ---
  {
   "cell_type": "markdown",
   "id": "ml_section_start",
   "metadata": {},
   "source": [
    "# 9. X√ÇY D·ª∞NG M√î H√åNH H·ªíI QUY V√Ä PH√ÇN LO·∫†I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml_data_prep_markdown",
   "metadata": {},
   "source": [
    "## 9.1. Chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh H·ªìi quy v√† Ph√¢n lo·∫°i\n",
    "\n",
    "S·ª≠ d·ª•ng `df_processed` l√†m c∆° s·ªü, t·∫°o m·ªôt b·∫£n sao `df_model_ml` ƒë·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu ri√™ng cho c√°c m√¥ h√¨nh n√†y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ml_data_prep_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_ml = df_processed.copy()\n",
    "\n",
    "# --- Ki·ªÉm tra v√† l√†m s·∫°ch l·∫°i c√°c c·ªôt s·ªë c·∫ßn thi·∫øt cho m√¥ h√¨nh ML ---\n",
    "cols_to_ensure_numeric = ['discounted_price', 'actual_price', 'discount_percentage', 'rating', 'rating_count']\n",
    "for col in cols_to_ensure_numeric:\n",
    "    if col in df_model_ml.columns:\n",
    "        df_model_ml[col] = pd.to_numeric(df_model_ml[col], errors='coerce')\n",
    "        if df_model_ml[col].isnull().any():\n",
    "             # ƒêi·ªÅn NaN b·∫±ng trung v·ªã cho c√°c c·ªôt s·ªë (ho·∫∑c trung b√¨nh)\n",
    "            df_model_ml[col].fillna(df_model_ml[col].median(), inplace=True)\n",
    "    else:\n",
    "        print(f\"C·∫£nh b√°o: C·ªôt {col} kh√¥ng t√¨m th·∫•y trong df_model_ml.\")\n",
    "\n",
    "# --- Feature Engineering cho m√¥ h√¨nh ph√¢n lo·∫°i ---\n",
    "if 'actual_price' in df_model_ml.columns and 'discounted_price' in df_model_ml.columns:\n",
    "    df_model_ml['price_diff'] = df_model_ml['actual_price'] - df_model_ml['discounted_price']\n",
    "    df_model_ml['price_ratio'] = np.where(df_model_ml['actual_price'] != 0, \n",
    "                                       df_model_ml['discounted_price'] / df_model_ml['actual_price'], \n",
    "                                       0) # Ho·∫∑c np.nan r·ªìi fillna sau\n",
    "else:\n",
    "    print(\"C·∫£nh b√°o: Thi·∫øu c·ªôt 'actual_price' ho·∫∑c 'discounted_price' ƒë·ªÉ t·∫°o features m·ªõi.\")\n",
    "    df_model_ml['price_diff'] = 0 # Gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu thi·∫øu c·ªôt\n",
    "    df_model_ml['price_ratio'] = 0\n",
    "\n",
    "# --- T·∫°o bi·∫øn m·ª•c ti√™u cho Ph√¢n lo·∫°i ---\n",
    "if 'rating' in df_model_ml.columns:\n",
    "    df_model_ml['rating_label'] = df_model_ml['rating'].apply(lambda x: 1 if x >= 4.0 else 0)\n",
    "else:\n",
    "    print(\"C·∫£nh b√°o: Thi·∫øu c·ªôt 'rating' ƒë·ªÉ t·∫°o nh√£n ph√¢n lo·∫°i.\")\n",
    "    df_model_ml['rating_label'] = 0 # Gi√° tr·ªã m·∫∑c ƒë·ªãnh\n",
    "\n",
    "# --- M√£ h√≥a One-Hot cho 'main_category' ---\n",
    "if 'main_category' in df_model_ml.columns:\n",
    "    df_model_ml = pd.get_dummies(df_model_ml, columns=['main_category'], prefix='cat', dummy_na=False)\n",
    "else:\n",
    "    print(\"C·∫£nh b√°o: Thi·∫øu c·ªôt 'main_category' ƒë·ªÉ m√£ h√≥a.\")\n",
    "\n",
    "# --- Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt cho m√¥ h√¨nh ---\n",
    "cols_to_drop = ['product_id', 'product_name', 'category', 'about_product', \n",
    "                  'user_id', 'user_name', 'review_id', 'review_title', \n",
    "                  'review_content', 'img_link', 'product_link', 'specific_category']\n",
    "df_model_ml.drop(columns=[col for col in cols_to_drop if col in df_model_ml.columns], inplace=True)\n",
    "\n",
    "# --- Lo·∫°i b·ªè c√°c d√≤ng c√≥ NaN c√≤n s√≥t l·∫°i (quan tr·ªçng tr∆∞·ªõc khi scale/hu·∫•n luy·ªán) ---\n",
    "print(f\"K√≠ch th∆∞·ªõc df_model_ml tr∆∞·ªõc khi dropna cu·ªëi c√πng: {df_model_ml.shape}\")\n",
    "df_model_ml.dropna(inplace=True)\n",
    "print(f\"K√≠ch th∆∞·ªõc df_model_ml sau khi dropna cu·ªëi c√πng: {df_model_ml.shape}\")\n",
    "\n",
    "df_model_ml.info()\n",
    "df_model_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regression_models_markdown",
   "metadata": {},
   "source": [
    "## 9.2. C√°c M√¥ H√¨nh H·ªìi Quy: D·ª± ƒëo√°n `discounted_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regression_data_prep_code",
   "metadata": {},
   "source": [
    "print(\"C√¢u h·ªèi h·ªìi quy: D·ª±a v√†o gi√° g·ªëc, % gi·∫£m gi√°, ƒëi·ªÉm ƒë√°nh gi√°, s·ªë l∆∞·ª£t ƒë√°nh gi√° v√† danh m·ª•c ch√≠nh, h√£y d·ª± ƒëo√°n gi√° ƒë√£ gi·∫£m c·ªßa s·∫£n ph·∫©m?\")\n",
    "\n",
    "# Ch·ªçn features cho h·ªìi quy\n",
    "cat_cols_reg = [col for col in df_model_ml.columns if col.startswith('cat_')]\n",
    "num_features_reg = ['actual_price', 'discount_percentage', 'rating', 'rating_count']\n",
    "X_reg_cols = num_features_reg + cat_cols_reg\n",
    "\n",
    "# ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c c·ªôt features t·ªìn t·∫°i\n",
    "X_reg_cols = [col for col in X_reg_cols if col in df_model_ml.columns]\n",
    "\n",
    "if 'discounted_price' in df_model_ml.columns and all(col in df_model_ml.columns for col in X_reg_cols):\n",
    "    X_reg = df_model_ml[X_reg_cols]\n",
    "    y_reg = df_model_ml['discounted_price']\n",
    "\n",
    "    X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Chu·∫©n h√≥a c√°c c·ªôt s·ªë\n",
    "    scaler_reg = StandardScaler()\n",
    "    X_reg_train_scaled = X_reg_train.copy()\n",
    "    X_reg_test_scaled = X_reg_test.copy()\n",
    "\n",
    "    X_reg_train_scaled[num_features_reg] = scaler_reg.fit_transform(X_reg_train[num_features_reg])\n",
    "    X_reg_test_scaled[num_features_reg] = scaler_reg.transform(X_reg_test[num_features_reg])\n",
    "\n",
    "    print(f\"S·ªë l∆∞·ª£ng features cho h·ªìi quy: {X_reg_train_scaled.shape[1]}\")\n",
    "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán h·ªìi quy: {X_reg_train_scaled.shape}, Nh√£n: {y_reg_train.shape}\")\n",
    "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra h·ªìi quy: {X_reg_test_scaled.shape}, Nh√£n: {y_reg_test.shape}\")\n",
    "else:\n",
    "    print(\"L·ªói: Thi·∫øu c·ªôt 'discounted_price' ho·∫∑c c√°c c·ªôt features c·∫ßn thi·∫øt cho m√¥ h√¨nh h·ªìi quy.\")\n",
    "    X_reg_train_scaled, X_reg_test_scaled, y_reg_train, y_reg_test = [None]*4 # ƒê·ªÉ tr√°nh l·ªói ·ªü c√°c cell sau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear_regression_markdown",
   "metadata": {},
   "source": [
    "### 9.2.1. H·ªìi quy Tuy·∫øn t√≠nh (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear_regression_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_reg_train_scaled is not None:\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_reg_train_scaled, y_reg_train)\n",
    "    y_reg_pred_lr = lr_model.predict(X_reg_test_scaled)\n",
    "\n",
    "    lr_rmse = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred_lr))\n",
    "    lr_r2 = r2_score(y_reg_test, y_reg_pred_lr)\n",
    "\n",
    "    print(f\"Linear Regression RMSE: {lr_rmse:.2f}\")\n",
    "    print(f\"Linear Regression R2 Score: {lr_r2:.4f}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ hu·∫•n luy·ªán Linear Regression do thi·∫øu d·ªØ li·ªáu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ann_regression_markdown",
   "metadata": {},
   "source": [
    "### 9.2.2. M·∫°ng Neural Nh√¢n t·∫°o (ANN) cho H·ªìi quy\n",
    "\n",
    "C·∫•u tr√∫c ANN cho h·ªìi quy:\n",
    "- **Input Layer:** S·ªë neuron b·∫±ng s·ªë l∆∞·ª£ng features.\n",
    "- **Hidden Layer 1:** 128 neurons, h√†m k√≠ch ho·∫°t 'relu', Dropout 0.3 (ngƒÉn overfitting).\n",
    "- **Hidden Layer 2:** 64 neurons, h√†m k√≠ch ho·∫°t 'relu', Dropout 0.3.\n",
    "- **Hidden Layer 3:** 32 neurons, h√†m k√≠ch ho·∫°t 'relu'.\n",
    "- **Output Layer:** 1 neuron (ƒë·ªÉ d·ª± ƒëo√°n gi√° tr·ªã `discounted_price`), kh√¥ng c√≥ h√†m k√≠ch ho·∫°t (ho·∫∑c 'linear').\n",
    "\n",
    "Bi√™n d·ªãch m√¥ h√¨nh:\n",
    "- **Optimizer:** 'adam'.\n",
    "- **Loss Function:** 'mean_squared_error' (MSE), ph√π h·ª£p cho b√†i to√°n h·ªìi quy.\n",
    "- **Metrics:** 'mean_absolute_error' (MAE) ƒë·ªÉ theo d√µi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ann_regression_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_reg_train_scaled is not None:\n",
    "    input_dim_reg = X_reg_train_scaled.shape[1]\n",
    "    \n",
    "    ann_reg_model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim_reg,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) # Output layer for regression\n",
    "    ])\n",
    "\n",
    "    ann_reg_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    ann_reg_model.summary()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history_reg = ann_reg_model.fit(\n",
    "        X_reg_train_scaled, y_reg_train,\n",
    "        epochs=100, \n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    y_reg_pred_ann = ann_reg_model.predict(X_reg_test_scaled).flatten()\n",
    "    ann_reg_rmse = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred_ann))\n",
    "    ann_reg_r2 = r2_score(y_reg_test, y_reg_pred_ann)\n",
    "\n",
    "    print(f\"ANN Regression RMSE: {ann_reg_rmse:.2f}\")\n",
    "    print(f\"ANN Regression R2 Score: {ann_reg_r2:.4f}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ hu·∫•n luy·ªán ANN cho H·ªìi quy do thi·∫øu d·ªØ li·ªáu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification_models_markdown",
   "metadata": {},
   "source": [
    "## 9.3. C√°c M√¥ H√¨nh Ph√¢n Lo·∫°i: D·ª± ƒëo√°n `rating_label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_data_prep_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"C√¢u h·ªèi ph√¢n lo·∫°i: D·ª±a v√†o c√°c ƒë·∫∑c ƒëi·ªÉm s·∫£n ph·∫©m, h√£y d·ª± ƒëo√°n xem s·∫£n ph·∫©m ƒë√≥ c√≥ rating cao (>=4) hay kh√¥ng?\")\n",
    "\n",
    "# Ch·ªçn features cho ph√¢n lo·∫°i\n",
    "cat_cols_cls = [col for col in df_model_ml.columns if col.startswith('cat_')]\n",
    "num_features_cls = ['discounted_price', 'actual_price', 'discount_percentage', 'rating_count', 'price_diff', 'price_ratio']\n",
    "X_cls_cols = num_features_cls + cat_cols_cls\n",
    "\n",
    "X_cls_cols = [col for col in X_cls_cols if col in df_model_ml.columns]\n",
    "\n",
    "if 'rating_label' in df_model_ml.columns and all(col in df_model_ml.columns for col in X_cls_cols):\n",
    "    X_cls = df_model_ml[X_cls_cols]\n",
    "    y_cls = df_model_ml['rating_label']\n",
    "\n",
    "    X_cls_train, X_cls_test, y_cls_train, y_cls_test = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42, stratify=y_cls)\n",
    "\n",
    "    # Chu·∫©n h√≥a c√°c c·ªôt s·ªë\n",
    "    scaler_cls = StandardScaler()\n",
    "    X_cls_train_scaled = X_cls_train.copy()\n",
    "    X_cls_test_scaled = X_cls_test.copy()\n",
    "\n",
    "    X_cls_train_scaled[num_features_cls] = scaler_cls.fit_transform(X_cls_train[num_features_cls])\n",
    "    X_cls_test_scaled[num_features_cls] = scaler_cls.transform(X_cls_test[num_features_cls])\n",
    "\n",
    "    print(f\"S·ªë l∆∞·ª£ng features cho ph√¢n lo·∫°i: {X_cls_train_scaled.shape[1]}\")\n",
    "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán ph√¢n lo·∫°i: {X_cls_train_scaled.shape}, Nh√£n: {y_cls_train.shape}\")\n",
    "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra ph√¢n lo·∫°i: {X_cls_test_scaled.shape}, Nh√£n: {y_cls_test.shape}\")\n",
    "    print(f\"Ph√¢n ph·ªëi nh√£n trong t·∫≠p hu·∫•n luy·ªán:\\n{y_cls_train.value_counts(normalize=True)}\")\n",
    "else:\n",
    "    print(\"L·ªói: Thi·∫øu c·ªôt 'rating_label' ho·∫∑c c√°c c·ªôt features c·∫ßn thi·∫øt cho m√¥ h√¨nh ph√¢n lo·∫°i.\")\n",
    "    X_cls_train_scaled, X_cls_test_scaled, y_cls_train, y_cls_test = [None]*4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logistic_regression_markdown",
   "metadata": {},
   "source": [
    "### 9.3.1. H·ªìi quy Logistic (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logistic_regression_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_cls_train_scaled is not None:\n",
    "    logreg_model = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear')\n",
    "    logreg_model.fit(X_cls_train_scaled, y_cls_train)\n",
    "    y_cls_pred_logreg = logreg_model.predict(X_cls_test_scaled)\n",
    "\n",
    "    logreg_accuracy = accuracy_score(y_cls_test, y_cls_pred_logreg)\n",
    "    print(f\"Logistic Regression Accuracy: {logreg_accuracy:.4f}\")\n",
    "    print(\"Classification Report (Logistic Regression):\\n\", classification_report(y_cls_test, y_cls_pred_logreg))\n",
    "    print(\"Confusion Matrix (Logistic Regression):\\n\", confusion_matrix(y_cls_test, y_cls_pred_logreg))\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ hu·∫•n luy·ªán Logistic Regression do thi·∫øu d·ªØ li·ªáu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ann_classification_markdown",
   "metadata": {},
   "source": [
    "### 9.3.2. M·∫°ng Neural Nh√¢n t·∫°o (ANN) cho Ph√¢n lo·∫°i\n",
    "\n",
    "C·∫•u tr√∫c ANN cho ph√¢n lo·∫°i nh·ªã ph√¢n:\n",
    "- **Input Layer:** S·ªë neuron b·∫±ng s·ªë l∆∞·ª£ng features.\n",
    "- **Hidden Layer 1:** 128 neurons, h√†m k√≠ch ho·∫°t 'relu', Dropout 0.4.\n",
    "- **Hidden Layer 2:** 64 neurons, h√†m k√≠ch ho·∫°t 'relu', Dropout 0.4.\n",
    "- **Hidden Layer 3:** 32 neurons, h√†m k√≠ch ho·∫°t 'relu'.\n",
    "- **Output Layer:** 1 neuron (cho ph√¢n lo·∫°i nh·ªã ph√¢n), h√†m k√≠ch ho·∫°t 'sigmoid' (ƒë∆∞a ra x√°c su·∫•t t·ª´ 0 ƒë·∫øn 1).\n",
    "\n",
    "Bi√™n d·ªãch m√¥ h√¨nh:\n",
    "- **Optimizer:** 'adam'.\n",
    "- **Loss Function:** 'binary_crossentropy', ph√π h·ª£p cho b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n.\n",
    "- **Metrics:** 'accuracy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ann_classification_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_cls_train_scaled is not None:\n",
    "    input_dim_cls = X_cls_train_scaled.shape[1]\n",
    "\n",
    "    ann_cls_model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim_cls,)),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid') # Output layer for binary classification\n",
    "    ])\n",
    "\n",
    "    ann_cls_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    ann_cls_model.summary()\n",
    "\n",
    "    early_stopping_cls = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history_cls = ann_cls_model.fit(\n",
    "        X_cls_train_scaled, y_cls_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping_cls],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    y_cls_pred_prob_ann = ann_cls_model.predict(X_cls_test_scaled)\n",
    "    y_cls_pred_ann = (y_cls_pred_prob_ann > 0.5).astype(int).flatten()\n",
    "\n",
    "    ann_cls_accuracy = accuracy_score(y_cls_test, y_cls_pred_ann)\n",
    "    print(f\"ANN Classification Accuracy: {ann_cls_accuracy:.4f}\")\n",
    "    print(\"Classification Report (ANN Classification):\\n\", classification_report(y_cls_test, y_cls_pred_ann))\n",
    "    print(\"Confusion Matrix (ANN Classification):\\n\", confusion_matrix(y_cls_test, y_cls_pred_ann))\n",
    "else:\n",
    "    print(\"Kh√¥ng th·ªÉ hu·∫•n luy·ªán ANN cho Ph√¢n lo·∫°i do thi·∫øu d·ªØ li·ªáu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_comparison_markdown",
   "metadata": {},
   "source": [
    "## 9.4. So s√°nh v√† K·∫øt lu·∫≠n M√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_comparison_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- K·∫æT QU·∫¢ M√î H√åNH H·ªíI QUY ---\")\n",
    "if 'lr_rmse' in locals(): # Ki·ªÉm tra bi·∫øn t·ªìn t·∫°i\n",
    "    print(f\"H·ªìi quy Tuy·∫øn t√≠nh - RMSE: {lr_rmse:.2f}, R2: {lr_r2:.4f}\")\n",
    "if 'ann_reg_rmse' in locals():\n",
    "    print(f\"ANN H·ªìi quy - RMSE: {ann_reg_rmse:.2f}, R2: {ann_reg_r2:.4f}\")\n",
    "\n",
    "print(\"\\n--- K·∫æT QU·∫¢ M√î H√åNH PH√ÇN LO·∫†I ---\")\n",
    "if 'logreg_accuracy' in locals():\n",
    "    print(f\"H·ªìi quy Logistic - Accuracy: {logreg_accuracy:.4f}\")\n",
    "    print(classification_report(y_cls_test, y_cls_pred_logreg))\n",
    "if 'ann_cls_accuracy' in locals():\n",
    "    print(f\"ANN Ph√¢n lo·∫°i - Accuracy: {ann_cls_accuracy:.4f}\")\n",
    "    print(classification_report(y_cls_test, y_cls_pred_ann))\n",
    "\n",
    "print(\"\\n--- NH·∫¨N X√âT ---\")\n",
    "print(\"1. H·ªìi quy (D·ª± ƒëo√°n discounted_price):\")\n",
    "print(\"   - M√¥ h√¨nh ANN th∆∞·ªùng c√≥ kh·∫£ nƒÉng h·ªçc c√°c m·ªëi quan h·ªá ph·ª©c t·∫°p t·ªët h∆°n h·ªìi quy tuy·∫øn t√≠nh, c√≥ th·ªÉ d·∫´n ƒë·∫øn RMSE th·∫•p h∆°n v√† R2 cao h∆°n n·∫øu ƒë∆∞·ª£c tinh ch·ªânh t·ªët.\")\n",
    "print(\"   - H·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n, d·ªÖ di·ªÖn gi·∫£i v√† l√† m·ªôt baseline t·ªët.\")\n",
    "print(\"2. Ph√¢n lo·∫°i (D·ª± ƒëo√°n rating_label):\")\n",
    "print(\"   - C·∫£ Logistic Regression v√† ANN ƒë·ªÅu l√† c√°c l·ª±a ch·ªçn ph·ªï bi·∫øn cho ph√¢n lo·∫°i nh·ªã ph√¢n.\")\n",
    "print(\"   - ANN c√≥ th·ªÉ ƒë·∫°t ƒë·ªô ch√≠nh x√°c cao h∆°n tr√™n c√°c t·∫≠p d·ªØ li·ªáu l·ªõn v√† c√≥ c·∫•u tr√∫c ph·ª©c t·∫°p, nh∆∞ng c·∫ßn nhi·ªÅu d·ªØ li·ªáu v√† th·ªùi gian hu·∫•n luy·ªán h∆°n, c≈©ng nh∆∞ d·ªÖ overfitting.\")\n",
    "print(\"   - Logistic Regression nhanh, d·ªÖ di·ªÖn gi·∫£i v√† ho·∫°t ƒë·ªông t·ªët tr√™n c√°c t·∫≠p d·ªØ li·ªáu c√≥ m·ªëi quan h·ªá tuy·∫øn t√≠nh ho·∫∑c g·∫ßn tuy·∫øn t√≠nh.\")\n",
    "print(\"3. H∆∞·ªõng c·∫£i thi·ªán:\")\n",
    "print(\"   - V·ªõi ANN: Th·ª≠ nghi·ªám c√°c ki·∫øn tr√∫c m·∫°ng kh√°c nhau (s·ªë l·ªõp, s·ªë neuron, h√†m k√≠ch ho·∫°t, t·ª∑ l·ªá dropout), c√°c optimizer kh√°c, v√† c√°c k·ªπ thu·∫≠t regularization.\")\n",
    "print(\"   - Feature Engineering: T·∫°o th√™m c√°c features c√≥ √Ω nghƒ©a t·ª´ d·ªØ li·ªáu hi·ªán c√≥.\")\n",
    "print(\"   - X·ª≠ l√Ω d·ªØ li·ªáu vƒÉn b·∫£n: S·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t NLP n√¢ng cao h∆°n (Word Embeddings nh∆∞ Word2Vec, FastText, ho·∫∑c c√°c m√¥ h√¨nh Transformer) cho c·ªôt 'about_product' ho·∫∑c 'review_content' ƒë·ªÉ tr√≠ch xu·∫•t features cho m√¥ h√¨nh.\")\n",
    "print(\"   - Cross-validation: S·ª≠ d·ª•ng k-fold cross-validation ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh m·ªôt c√°ch ƒë√°ng tin c·∫≠y h∆°n.\")\n",
    "print(\"   - So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c: Random Forest, Gradient Boosting, SVM...\")"
   ]
  }
  # --- K·∫øt th√∫c ph·∫ßn M·ªöI TH√äM V√ÄO ---
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8" # Ho·∫∑c phi√™n b·∫£n Python c·ªßa b·∫°n
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}